<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision for Autonomous Vehicles: An Introduction | Solve for AI</title>
    <meta name="description" content="Discover how Computer Vision is revolutionizing autonomous vehicles. Learn about object detection, semantic segmentation, and real-time processing.">
    <meta name="keywords" content="Computer Vision, Autonomous Vehicles, Object Detection, Semantic Segmentation">
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/tutorials.css">
</head>
<body>
    <header>
        <div class="container header-container">
            <div class="logo">
                <a href="../index.html">Solve<span> for AI</span></a>
            </div>
            <nav>
                <ul>
                    <li><a href="../index.html">Home</a></li>
                    <li><a href="../tutorials.html" class="active">Tutorials</a></li>
                    <li><a href="../learning-paths.html">Learning Paths</a></li>
                    <li><a href="../tools.html">AI Tools</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="tutorial-container">
        <article class="tutorial-content">
            <header class="tutorial-header">
                <h1>Computer Vision for Autonomous Vehicles: An Introduction</h1>
                <div class="tutorial-meta">
                    <span class="category">Computer-vision</span>
                    <span class="reading-time">18 min read</span>
                    <span class="publish-date">Updated: June 18, 2025</span>
                </div>

                <div class="author-info">
                    <img src="https://randomuser.me/api/portraits/men/32.jpg" alt="Author photo" class="author-photo">
                    <div class="author-details">
                        <span class="author-name">AI Content Team</span>
                        <span class="author-title">AI Research Engineer</span>
                    </div>
                </div>

                <img src="https://images.unsplash.com/photo-1550751827-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=1200&q=80" alt="Computer Vision for Autonomous Vehicles: An Introduction" class="hero-image">

                <div class="table-of-contents">
                    <h3>Table of Contents</h3>
                    <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#fundamentals-of-image-processing">Fundamentals of Image Processing</a></li>
        <ul>
            <li><a href="#fundamentals-of-image-processing-understanding-image-data-pixels-color-models-and-resolutions">Understanding image data: pixels, color models, and resolutions</a></li>
            <li><a href="#fundamentals-of-image-processing-pre-processing-techniques-noise-reduction-normalization-and-filtering">Pre-processing techniques: noise reduction, normalization, and filtering</a></li>
            <li><a href="#fundamentals-of-image-processing-image-enhancement-edge-detection-histogram-equalization">Image enhancement: edge detection, histogram equalization</a></li>
        </ul>
    <li><a href="#object-detection-techniques">Object Detection Techniques</a></li>
        <ul>
            <li><a href="#object-detection-techniques-introduction-to-object-detection-concepts-and-importance">Introduction to object detection: concepts and importance</a></li>
            <li><a href="#object-detection-techniques-traditional-methods-vs-deep-learning-approaches">Traditional methods vs deep learning approaches</a></li>
            <li><a href="#object-detection-techniques-implementing-object-detection-using-opencv-and-python">Implementing object detection using OpenCV and Python</a></li>
            <li><a href="#object-detection-techniques-common-pitfalls-in-object-detection-and-how-to-overcome-them">Common pitfalls in object detection and how to overcome them</a></li>
        </ul>
    <li><a href="#semantic-segmentation-and-depth-estimation">Semantic Segmentation and Depth Estimation</a></li>
        <ul>
            <li><a href="#semantic-segmentation-and-depth-estimation-understanding-semantic-segmentation-definition-and-applications">Understanding semantic segmentation: definition and applications</a></li>
            <li><a href="#semantic-segmentation-and-depth-estimation-techniques-for-semantic-segmentation-from-classical-methods-to-cnns">Techniques for semantic segmentation: from classical methods to CNNs</a></li>
            <li><a href="#semantic-segmentation-and-depth-estimation-depth-estimation-basics-and-its-significance-in-autonomous-vehicles">Depth estimation basics and its significance in autonomous vehicles</a></li>
            <li><a href="#semantic-segmentation-and-depth-estimation-code-example-semantic-segmentation-with-tensorflow">Code example: Semantic segmentation with TensorFlow</a></li>
        </ul>
    <li><a href="#real-time-processing-for-autonomous-driving">Real-Time Processing for Autonomous Driving</a></li>
        <ul>
            <li><a href="#real-time-processing-for-autonomous-driving-challenges-of-real-time-computer-vision-systems">Challenges of real-time computer vision systems</a></li>
            <li><a href="#real-time-processing-for-autonomous-driving-technologies-enabling-real-time-processing-gpus-fpgas-and-asics">Technologies enabling real-time processing: GPUs, FPGAs, and ASICs</a></li>
            <li><a href="#real-time-processing-for-autonomous-driving-optimizing-algorithms-for-real-time-performance">Optimizing algorithms for real-time performance</a></li>
            <li><a href="#real-time-processing-for-autonomous-driving-case-study-real-time-processing-in-a-production-level-autonomous-vehicle">Case study: Real-time processing in a production-level autonomous vehicle</a></li>
        </ul>
    <li><a href="#best-practices-ethics-and-future-trends">Best Practices, Ethics, and Future Trends</a></li>
        <ul>
            <li><a href="#best-practices-ethics-and-future-trends-adhering-to-best-practices-in-computer-vision-for-safety-and-reliability">Adhering to best practices in computer vision for safety and reliability</a></li>
            <li><a href="#best-practices-ethics-and-future-trends-ethical-considerations-in-the-deployment-of-computer-vision-systems">Ethical considerations in the deployment of computer vision systems</a></li>
            <li><a href="#best-practices-ethics-and-future-trends-emerging-trends-and-technologies-in-computer-vision-for-autonomous-vehicles">Emerging trends and technologies in computer vision for autonomous vehicles</a></li>
            <li><a href="#best-practices-ethics-and-future-trends-preparing-for-the-future-skills-technologies-and-knowledge">Preparing for the future: skills, technologies, and knowledge</a></li>
        </ul>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#code-examples">Code Examples</a></li>
</ul>
                </div>
            </header>

            <section class="tutorial-body">
                
                <section id="introduction">
                    <h2>Introduction</h2>
                    <p># Welcome to "Computer Vision for Autonomous Vehicles: An Introduction"</p><p>## Unlocking the Eyes of the Future</p><p>Imagine a world where your car not only drives you but also sees and understands the road as well as, if not better than, you do. This isn't a scene from a sci-fi movie; it's the reality that <strong>Computer Vision</strong> is creating for <strong>Autonomous Vehicles</strong>. As we stand on the brink of revolutionary changes in transportation, understanding how these vehicles perceive and interact with their environment is not just fascinating—it's essential.</p><p>In this intermediate-level tutorial, you will dive deep into the core technologies that enable autonomous vehicles to navigate the complexities of real-world environments. You'll uncover how these machines use computer vision to detect and respond to objects, make critical driving decisions, and ultimately, reshape our ideas of mobility and safety.</p><p>## What You Will Learn</p><p>This tutorial is designed to provide you with a comprehensive understanding of the key aspects of computer vision as applied in autonomous driving. Here’s what we’ll cover:</p><p>- <strong>Object Detection</strong>: Learn how autonomous vehicles identify and categorize objects like other vehicles, pedestrians, and traffic signs in real-time.<br>- <strong>Semantic Segmentation</strong>: Discover how vehicles interpret and analyze every pixel from the car’s cameras to understand and segment different parts of the road scene.<br>- <strong>Real-Time Processing</strong>: Explore the technologies that allow for immediate processing of visual data, enabling quick decision-making on the road.</p><p>## Prerequisites</p><p>To get the most out of this tutorial, you should have:<br>- A basic understanding of programming, preferably in Python<br>- Familiarity with fundamental concepts in machine learning and image processing<br>- An enthusiasm for delving into advanced technology applications</p><p>No prior experience in autonomous vehicle technology is required, but any background knowledge in artificial intelligence or robotics will enhance your learning experience.</p><p>## Tutorial Overview</p><p>Throughout this tutorial, we will interlace theoretical knowledge with practical examples and hands-on exercises. Starting with an introduction to the basic principles of computer vision, we will progressively delve into more complex systems and algorithms that drive autonomous vehicles. By the end of this series, you will not only grasp how these vehicles 'see', but also appreciate the incredible potential and challenges of implementing these systems in real-world scenarios.</p><p>Prepare to embark on a journey through the eyes of autonomous vehicles. Enhance your skills, expand your knowledge, and get ready to be part of a future where cars are more than just machines—they are intelligent companions on the road. Join us as we explore the visionary world of computer vision in autonomous vehicles!</p>
                </section>
                
                <div class="advertisement">
                    <h4>Your Ad Could Be Here</h4>
                    <p>This is where a real content advertisement would appear.</p>
                    <a href="#" class="ad-link">Learn More</a>
                </div>
                

                
                  <section id="fundamentals-of-image-processing">
                      <h2>Fundamentals of Image Processing</h2>
                      <img src="https://images.unsplash.com/photo-1550000000000-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Illustration for Fundamentals of Image Processing" class="section-image">
                      <p>## Fundamentals of Image Processing</p><p>Image processing forms the backbone of computer vision applications, especially in the context of autonomous vehicles. This section delves into the basic concepts and techniques essential for understanding and manipulating image data effectively.</p><p>### 1. Understanding Image Data: Pixels, Color Models, and Resolutions</p><p>#### <strong>Pixels</strong><br>At its core, an image is a matrix of pixels, where each pixel represents a tiny area of the visual content. In the context of autonomous vehicles, understanding pixel values is crucial for tasks like object detection and semantic segmentation.</p><p>#### <strong>Color Models</strong><br>Images can be represented in various color models, each serving different purposes:<br>- <strong>RGB (Red, Green, Blue)</strong>: The most common model used in digital images, ideal for display on screens.<br>- <strong>HSV (Hue, Saturation, Value)</strong>: Often more useful for color filtering operations.<br>- <strong>Grayscale</strong>: Reduces the image to shades of gray, simplifying the data and reducing computational requirements, which is beneficial for certain applications in autonomous vehicles like night vision processing.</p><p><code></code>`python<br># Converting an RGB image to Grayscale using Python's PIL library<br>from PIL import Image<br>img = Image.open('path_to_image.jpg').convert('L')<br>img.show()<br><code></code>`</p><p>#### <strong>Resolutions</strong><br>Resolution refers to the number of pixels that make up an image, typically presented as width x height. Higher resolutions mean more detail but also more computational expense. For autonomous vehicles, a balance must be struck between resolution and real-time processing capabilities.</p><p>### 2. Pre-processing Techniques: Noise Reduction, Normalization, and Filtering</p><p>#### <strong>Noise Reduction</strong><br>Noise in images, especially those captured by vehicle cameras, can significantly affect the performance of computer vision algorithms. Techniques such as Gaussian blurring can help reduce noise:</p><p><code></code>`python<br># Applying Gaussian Blur using OpenCV<br>import cv2<br>image = cv2.imread('noisy_image.jpg')<br>smoothed = cv2.GaussianBlur(image, (5, 5), 0)<br><code></code>`</p><p>#### <strong>Normalization</strong><br>Normalization adjusts pixel values across an image to a common scale, improving the consistency of input data for machine learning models.</p><p><code></code>`python<br># Normalizing image data<br>normalized_image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)<br><code></code>`</p><p>#### <strong>Filtering</strong><br>Filtering can enhance image features for better interpretation by algorithms. A simple example is the Sobel filter that highlights vertical or horizontal edges—an essential feature in lane detection.</p><p><code></code>`python<br># Applying Sobel filter to detect vertical edges<br>sobel_vertical = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)<br><code></code>`</p><p>### 3. Image Enhancement: Edge Detection, Histogram Equalization</p><p>#### <strong>Edge Detection</strong><br>Edge detection is pivotal in delineating boundaries within an image, crucial for object detection in autonomous vehicles. The Canny edge detector is a popular choice due to its accuracy.</p><p><code></code>`python<br># Canny edge detection<br>edges = cv2.Canny(image, threshold1=50, threshold2=150)<br><code></code>`</p><p>#### <strong>Histogram Equalization</strong><br>This technique improves the contrast of an image by stretching out the intensity range. It's particularly useful in low-light conditions often encountered by autonomous vehicles.</p><p><code></code>`python<br># Applying Histogram Equalization<br>equalized_image = cv2.equalizeHist(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY))<br><code></code>`</p><p>### Best Practices and Practical Tips</p><p>- <strong>Choose the right color model</strong> for specific tasks: RGB for color-based object detection and HSV for scenarios involving varying lighting conditions.<br>- <strong>Regularly calibrate your camera</strong> to maintain accuracy in capturing images.<br>- Use <strong>adaptive methods</strong> in noise reduction and histogram equalization to better cope with different lighting conditions and dynamic environments typical in autonomous vehicle settings.</p><p>By mastering these fundamentals of image processing, developers can enhance their computer vision systems' reliability and efficiency, ensuring safer navigation for autonomous vehicles.</p>
                      
                      <h3 id="fundamentals-of-image-processing-understanding-image-data-pixels-color-models-and-resolutions">Understanding image data: pixels, color models, and resolutions</h3><h3 id="fundamentals-of-image-processing-pre-processing-techniques-noise-reduction-normalization-and-filtering">Pre-processing techniques: noise reduction, normalization, and filtering</h3><h3 id="fundamentals-of-image-processing-image-enhancement-edge-detection-histogram-equalization">Image enhancement: edge detection, histogram equalization</h3>
                  </section>
                  
                  
                  <section id="object-detection-techniques">
                      <h2>Object Detection Techniques</h2>
                      <img src="https://images.unsplash.com/photo-1550000001000-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Illustration for Object Detection Techniques" class="section-image">
                      <p># Object Detection Techniques</p><p>## 1. Introduction to Object Detection: Concepts and Importance</p><p>Object detection is a fundamental aspect of computer vision that involves identifying and locating objects within an image or a video. This technique not only classifies what objects are present in an image but also provides the bounding boxes indicating their positions. For autonomous vehicles, object detection is crucial as it helps in recognizing pedestrians, vehicles, traffic signs, and other vital elements to navigate safely.</p><p>Object detection enables semantic segmentation, where objects are segmented according to predefined categories, aiding in detailed scene understanding which is vital for the decision-making processes in autonomous driving systems.</p><p>## 2. Traditional Methods vs Deep Learning Approaches</p><p>### Traditional Methods<br>Initially, object detection relied on methods like Haar cascades and Histogram of Oriented Gradients (HOG). These techniques involved manual feature extraction and were effective for simple tasks but struggled with complex scenarios and variances in object appearances, scales, and rotations.</p><p>### Deep Learning Approaches<br>The advent of deep learning has revolutionized object detection. Neural networks, particularly Convolutional Neural Networks (CNNs), are now at the core of most state-of-the-art object detection models due to their ability to learn hierarchical features automatically. Models like R-CNN, YOLO (You Only Look Once), and SSD (Single Shot MultiDetector) have significantly improved the accuracy and speed of object detection systems, making them ideal for real-time applications such as autonomous driving.</p><p>## 3. Implementing Object Detection Using OpenCV and Python</p><p>OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning software library that provides a common infrastructure for computer vision applications. Below is a basic example of implementing object detection using OpenCV and a pre-trained YOLO model in Python:</p><p><code></code>`python<br>import cv2<br>import numpy as np</p><p># Load YOLO<br>net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")<br>classes = []<br>with open("coco.names", "r") as f:<br>    classes = [line.strip() for line in f.readlines()]</p><p>layer_names = net.getLayerNames()<br>output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]</p><p># Loading image<br>img = cv2.imread("image.jpg")<br>img = cv2.resize(img, None, fx=0.4, fy=0.4)<br>height, width, channels = img.shape</p><p># Detecting objects<br>blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)<br>net.setInput(blob)<br>outs = net.forward(output_layers)</p><p># Information to show on the screen (class id and confidence)<br>class_ids = []<br>confidences = []<br>boxes = []<br>for out in outs:<br>    for detection in out:<br>        scores = detection[5:]<br>        class_id = np.argmax(scores)<br>        confidence = scores[class_id]<br>        if confidence > 0.5:<br>            # Object detected<br>            center_x = int(detection[0] * width)<br>            center_y = int(detection[1] * height)<br>            w = int(detection[2] * width)<br>            h = int(detection[3] * height)</p><p>            # Rectangle coordinates<br>            x = int(center_x - w / 2)<br>            y = int(center_y - h / 2)</p><p>            boxes.append([x, y, w, h])<br>            confidences.append(float(confidence))<br>            class_ids.append(class_id)</p><p># Apply Non-max suppression<br>indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)<br>for i in range(len(boxes)):<br>    if i in indexes:<br>        x, y, w, h = boxes[i]<br>        label = str(classes[class_ids[i]])<br>        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)<br>        cv2.putText(img, label, (x, y + 30), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 3)<br>        <br>cv2.imshow("Image", img)<br>cv2.waitKey(0)<br>cv2.destroyAllWindows()<br><code></code>`</p><p>In this code snippet, we load a pre-trained YOLO model and use it to detect objects in an image. We then apply non-max suppression to refine the bounding boxes around detected objects.</p><p>## 4. Common Pitfalls in Object Detection and How to Overcome Them</p><p>### Variability in Environmental Conditions<br>Autonomous vehicles must operate reliably under various lighting conditions and weather. Deep learning models can be sensitive to such changes. <strong>Solution:</strong> Augment training data with images captured under diverse conditions or use domain adaptation techniques.</p><p>### Real-Time Performance<br>For autonomous vehicles, decisions must be made swiftly. <strong>Solution:</strong> Optimize deep learning models for inference speed without significantly compromising accuracy. Techniques such as model pruning or using efficient architectures like MobileNet or SqueezeNet can be beneficial.</p><p>### Handling Occlusions<br>Objects of interest might be partially occluded by other objects. <strong>Solution:</strong> Incorporate training samples with occluded objects and perhaps use context-aware models that infer partially visible objects based on the surroundings.</p><p>### Scalability Across Different Geographies<br>Objects can look different in various parts of the world. <strong>Solution:</strong> Ensure the training dataset is geographically diverse or fine-tune models on region-specific data sets.</p><p>By understanding these pitfalls and strategically addressing them, developers can enhance the robustness and accuracy of object detection systems in autonomous vehicles.</p>
                      
                      <h3 id="object-detection-techniques-introduction-to-object-detection-concepts-and-importance">Introduction to object detection: concepts and importance</h3><h3 id="object-detection-techniques-traditional-methods-vs-deep-learning-approaches">Traditional methods vs deep learning approaches</h3><h3 id="object-detection-techniques-implementing-object-detection-using-opencv-and-python">Implementing object detection using OpenCV and Python</h3><h3 id="object-detection-techniques-common-pitfalls-in-object-detection-and-how-to-overcome-them">Common pitfalls in object detection and how to overcome them</h3>
                  </section>
                  
                  
                  <section id="semantic-segmentation-and-depth-estimation">
                      <h2>Semantic Segmentation and Depth Estimation</h2>
                      <img src="https://images.unsplash.com/photo-1550000002000-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Illustration for Semantic Segmentation and Depth Estimation" class="section-image">
                      <p># Semantic Segmentation and Depth Estimation</p><p>In the realm of Computer Vision for Autonomous Vehicles, two critical techniques—semantic segmentation and depth estimation—play pivotal roles. This section delves into these concepts, providing a clear understanding, discussing various techniques, and offering a practical example using TensorFlow.</p><p>## 1. Understanding Semantic Segmentation: Definition and Applications</p><p>Semantic segmentation refers to the process of partitioning an image into segments, where each segment corresponds to different objects or parts of objects. Unlike object detection, which identifies object bounding boxes, semantic segmentation provides a pixel-wise classification, making it crucial for detailed scene understanding.</p><p>### Applications in Autonomous Vehicles:<br>- <strong>Road Scene Analysis:</strong> Identifies drivable surfaces, lanes, sidewalks, and obstacles.<br>- <strong>Enhanced Navigation and Safety:</strong> Helps in precise maneuvering and obstacle avoidance by understanding the environment at a granular level.</p><p>Semantic segmentation extends beyond autonomous driving to medical imaging, aerial image analysis, and more, highlighting its versatility in various fields of Computer Vision.</p><p>## 2. Techniques for Semantic Segmentation: From Classical Methods to CNNs</p><p>Initially, semantic segmentation relied on classical image processing techniques like thresholding and edge detection. However, these methods struggled with complex scenes and variability in lighting conditions.</p><p>### Evolution to Deep Learning:<br>With the advent of Convolutional Neural Networks (CNNs), semantic segmentation has seen significant improvements. CNNs like FCN (Fully Convolutional Networks) and architectures such as U-Net and DeepLab have set new benchmarks by learning hierarchical features effective for diverse scenarios.</p><p>- <strong>FCN:</strong> Replaces fully connected layers with convolutional ones, enabling end-to-end training and prediction.<br>- <strong>U-Net:</strong> Known for its effectiveness in medical image segmentation due to its symmetric expanding path that captures context and enables precise localization.<br>- <strong>DeepLab:</strong> Utilizes atrous convolutions to control the resolution at which feature responses are computed within deep convolutional neural networks.</p><p>## 3. Depth Estimation Basics and Its Significance in Autonomous Vehicles</p><p>Depth estimation involves predicting the distance of objects from the observer, crucial for 3D scene understanding. In autonomous vehicles, depth perception is vital for:<br>- <strong>Obstacle Avoidance:</strong> Determining the distance to nearby vehicles and pedestrians to avoid collisions.<br>- <strong>Path Planning:</strong> Ensuring the vehicle has enough space to maneuver or stop.</p><p>### Techniques:<br>- <strong>Stereo Vision:</strong> Uses two cameras to simulate human binocular vision, deriving depth information from the disparity between the two images.<br>- <strong>Monocular Depth Estimation:</strong> Machine learning models predict depth from a single image, using patterns learned from large datasets.</p><p>## 4. Code Example: Semantic Segmentation with TensorFlow</p><p>Let's explore a practical example of implementing semantic segmentation using TensorFlow and a pre-trained DeepLab model. This example requires TensorFlow to be installed in your Python environment.</p><p><code></code>`python<br>import tensorflow as tf<br>import numpy as np<br>import matplotlib.pyplot as plt<br>from tensorflow.keras.applications import ResNet50<br>from tensorflow.keras.layers import Conv2D, UpSampling2D</p><p># Load a pre-trained DeepLab model<br>model = tf.keras.models.load_model('deeplab_model.h5')</p><p>def semantic_segmentation(image_path):<br>    """Function to perform semantic segmentation on an input image."""<br>    image = tf.image.decode_jpeg(tf.io.read_file(image_path))<br>    resized_image = tf.image.resize(image, (512, 512))<br>    input_tensor = resized_image[tf.newaxis, ...]</p><p>    # Model inference<br>    predictions = model.predict(input_tensor)</p><p>    # Visualize the segmentation output<br>    plt.figure(figsize=(12, 8))<br>    plt.subplot(1, 2, 1)<br>    plt.title('Original Image')<br>    plt.imshow(image.numpy().astype("uint8"))<br>    plt.subplot(1, 2, 2)<br>    plt.title('Segmented Image')<br>    plt.imshow(np.argmax(predictions[0], axis=-1))<br>    plt.show()</p><p># Example usage<br>semantic_segmentation('path_to_image.jpg')<br><code></code>`</p><p>### Best Practices:<br>- <strong>Data Preprocessing:</strong> Normalize images before feeding them into the model for consistent performance.<br>- <strong>Model Tuning:</strong> Fine-tune the model on specific datasets related to autonomous driving for more accurate results.</p><p>This code demonstrates how to use a pre-trained DeepLab model for semantic segmentation. It loads an image, resizes it for the model input, performs inference, and visualizes the segmented output alongside the original image.</p><p>## Conclusion</p><p>Semantic segmentation and depth estimation are indispensable in developing robust computer vision systems for autonomous vehicles. By leveraging advanced CNN architectures and depth estimation techniques, developers can significantly enhance the perception capabilities of autonomous systems.</p>
                      
                      <h3 id="semantic-segmentation-and-depth-estimation-understanding-semantic-segmentation-definition-and-applications">Understanding semantic segmentation: definition and applications</h3><h3 id="semantic-segmentation-and-depth-estimation-techniques-for-semantic-segmentation-from-classical-methods-to-cnns">Techniques for semantic segmentation: from classical methods to CNNs</h3><h3 id="semantic-segmentation-and-depth-estimation-depth-estimation-basics-and-its-significance-in-autonomous-vehicles">Depth estimation basics and its significance in autonomous vehicles</h3><h3 id="semantic-segmentation-and-depth-estimation-code-example-semantic-segmentation-with-tensorflow">Code example: Semantic segmentation with TensorFlow</h3>
                  </section>
                  
                  <div class="advertisement">
                      <h4>Your Ad Could Be Here</h4>
                      <p>This is where a real content advertisement would appear.</p>
                      <a href="#" class="ad-link">Learn More</a>
                  </div>
                  
                  
                  <section id="real-time-processing-for-autonomous-driving">
                      <h2>Real-Time Processing for Autonomous Driving</h2>
                      <img src="https://images.unsplash.com/photo-1550000003000-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Illustration for Real-Time Processing for Autonomous Driving" class="section-image">
                      <p># Real-Time Processing for Autonomous Driving</p><p>In the realm of autonomous vehicles, real-time processing is a critical component that ensures the safety and efficiency of operations. This section explores the intricacies of real-time computer vision systems used in autonomous driving, covering technological enablers, optimization strategies, and a practical case study.</p><p>## 1. Challenges of Real-Time Computer Vision Systems</p><p>Real-time computer vision is pivotal for autonomous vehicles, which must interpret and respond to their environment swiftly and accurately. The primary challenges include:</p><p>- <strong>High Data Throughput</strong>: Autonomous vehicles generate massive amounts of data from sensors and cameras. Processing this data in real-time demands substantial computational resources.<br>- <strong>Latency Requirements</strong>: Delays in processing can lead to missed cues or outdated information, potentially causing accidents. Keeping latency minimal is crucial for timely decision-making.<br>- <strong>Environmental Variability</strong>: Varying lighting conditions, weather, and unexpected obstacles require robust and adaptable vision algorithms.<br>- <strong>Resource Constraints</strong>: Despite the need for powerful processing capabilities, systems must also be energy-efficient and compact enough to fit within a vehicle.</p><p>These challenges necessitate advanced technologies and optimization techniques to ensure that computer vision systems can function effectively under real-world conditions.</p><p>## 2. Technologies Enabling Real-Time Processing: GPUs, FPGAs, and ASICs</p><p>To meet the demands of real-time processing, several technological solutions are employed:</p><p>- <strong>Graphics Processing Units (GPUs)</strong>: Originally designed for rendering graphics, GPUs are ideal for parallel tasks like image processing and deep learning. They accelerate tasks such as object detection and semantic segmentation, crucial for interpreting road scenes.</p><p><code></code>`python<br># Example: Using a GPU with CUDA in Python for image processing<br>import cv2<br># Assuming 'frame' is a captured image from a video stream<br>gpu_frame = cv2.cuda_GpuMat()  # Create a GPU matrix<br>gpu_frame.upload(frame)        # Upload the frame to GPU memory<br><code></code>`</p><p>- <strong>Field-Programmable Gate Arrays (FPGAs)</strong>: FPGAs provide a customizable hardware platform, allowing for tailored optimizations. They offer lower latency than GPUs, making them suitable for time-critical applications.</p><p>- <strong>Application-Specific Integrated Circuits (ASICs)</strong>: ASICs are custom chips designed specifically for a particular application. In autonomous vehicles, ASICs are optimized for high-performance tasks like real-time sensor data processing.</p><p>Each of these technologies has its strengths, and often, a combination is used to optimize both performance and power efficiency.</p><p>## 3. Optimizing Algorithms for Real-Time Performance</p><p>Optimization of computer vision algorithms is essential for efficient real-time processing. Key strategies include:</p><p>- <strong>Algorithm Simplification</strong>: Streamlining algorithms to reduce computational complexity without sacrificing accuracy.<br>- <strong>Data Reduction</strong>: Applying techniques such as region of interest (ROI) selection or downsampling to decrease the amount of data processed.<br>- <strong>Parallel Processing</strong>: Implementing algorithms in a manner that leverages parallel architectures of GPUs or FPGAs.<br>- <strong>Network Pruning and Quantization</strong>: For deep learning models, reducing the number of parameters and using lower precision arithmetic can significantly speed up computations.</p><p>These optimizations help maintain the balance between performance and computational demand, crucial for the deployment in resource-constrained environments like vehicles.</p><p>## 4. Case Study: Real-Time Processing in a Production-Level Autonomous Vehicle</p><p>Consider a production-level autonomous vehicle equipped with multiple cameras and sensors. The vehicle uses a combination of GPUs and ASICs to handle various tasks:</p><p>- GPUs are primarily used for initial image processing tasks, including object detection and semantic segmentation, utilizing frameworks like TensorFlow or PyTorch.<br>- The processed data is then handed over to an ASIC designed specifically for tracking detected objects and making real-time driving decisions.</p><p>This setup exemplifies how different technologies can be integrated to handle specific parts of the processing pipeline effectively. By optimizing each stage of the pipeline, from data acquisition to decision-making, the system manages to meet stringent real-time performance criteria necessary for safe autonomous driving.</p><p>### Practical Tips:<br>- Always benchmark performance on the target hardware.<br>- Regularly update algorithms to leverage improvements in processing technologies and techniques.<br>- Consider the trade-off between accuracy and speed; sometimes slight reductions in accuracy can lead to significant gains in performance.</p><p>In conclusion, real-time processing in autonomous vehicles is a complex but manageable challenge that requires a multifaceted approach involving advanced technologies, strategic optimizations, and continuous improvements. By understanding and applying these principles, developers can create efficient systems capable of operating safely and effectively in dynamic environments.</p>
                      
                      <h3 id="real-time-processing-for-autonomous-driving-challenges-of-real-time-computer-vision-systems">Challenges of real-time computer vision systems</h3><h3 id="real-time-processing-for-autonomous-driving-technologies-enabling-real-time-processing-gpus-fpgas-and-asics">Technologies enabling real-time processing: GPUs, FPGAs, and ASICs</h3><h3 id="real-time-processing-for-autonomous-driving-optimizing-algorithms-for-real-time-performance">Optimizing algorithms for real-time performance</h3><h3 id="real-time-processing-for-autonomous-driving-case-study-real-time-processing-in-a-production-level-autonomous-vehicle">Case study: Real-time processing in a production-level autonomous vehicle</h3>
                  </section>
                  
                  
                  <section id="best-practices-ethics-and-future-trends">
                      <h2>Best Practices, Ethics, and Future Trends</h2>
                      <img src="https://images.unsplash.com/photo-1550000004000-4bd374c3f58b?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80" alt="Illustration for Best Practices, Ethics, and Future Trends" class="section-image">
                      <p># Best Practices, Ethics, and Future Trends in Computer Vision for Autonomous Vehicles</p><p>## Adhering to Best Practices in Computer Vision for Safety and Reliability</p><p>In the realm of autonomous vehicles (AVs), the safety and reliability of computer vision systems are paramount. These systems, which include technologies like object detection and semantic segmentation, are critical for interpreting and understanding the vehicle's surroundings.</p><p>### Best Practices</p><p>1. <strong>Data Diversity</strong>: Ensure the training datasets are diverse and cover a wide range of scenarios that an AV might encounter. This includes varying weather conditions, different times of the day, and a multitude of environments and traffic situations.</p><p>   <code></code>`python<br>   # Example: Ensuring data diversity in datasets<br>   import numpy as np<br>   import cv2</p><p>   def augment_image(image):<br>       # Rotate image<br>       center = tuple(np.array(image.shape[1::-1]) / 2)<br>       rot_mat = cv2.getRotationMatrix2D(center, angle=np.random.uniform(-10, 10), scale=1.0)<br>       return cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)</p><p>   # Apply augmentation to increase dataset diversity<br>   augmented_images = [augment_image(img) for img in original_dataset]<br>   <code></code>`</p><p>2. <strong>Continuous Testing and Validation</strong>: Regularly test the computer vision algorithms against new, unseen datasets to evaluate their performance and robustness.</p><p>3. <strong>Redundancy</strong>: Implement multiple fail-safe mechanisms and redundant systems to ensure that if one component fails, others can take over seamlessly.</p><p>### Reliability Measures</p><p>- Implement real-time monitoring systems to track the performance and decision-making processes of computer vision systems in AVs.<br>- Use rigorous version control and thorough documentation for every update or modification in the system.</p><p>## Ethical Considerations in the Deployment of Computer Vision Systems</p><p>Ethical deployment of computer vision in autonomous vehicles involves several key considerations:</p><p>1. <strong>Privacy</strong>: Techniques such as anonymization should be used when handling data captured by AVs to ensure individual privacy is maintained.</p><p>2. <strong>Bias Mitigation</strong>: It is crucial to continuously work on detecting and mitigating biases in AI models that could lead to discriminatory practices. For instance, ensuring that the object detection systems perform equally well across different demographic groups.</p><p>3. <strong>Transparency</strong>: Maintaining transparency with users about how the data is used and how decisions are made by the autonomous systems.</p><p>4. <strong>Accountability</strong>: Establishing clear guidelines on the accountability of decisions made by AVs, especially in the case of accidents or failures.</p><p>## Emerging Trends and Technologies in Computer Vision for Autonomous Vehicles</p><p>The field of computer vision for AVs is rapidly evolving with several emerging trends:</p><p>1. <strong>Deep Learning Enhancements</strong>: Innovations in neural network architectures continue to improve tasks like object detection and semantic segmentation. For example, the integration of Capsule Networks can enhance the way spatial relationships are understood by AVs.</p><p>2. <strong>Edge Computing</strong>: Processing data directly on local devices (edge computing) reduces latency, which is crucial for the real-time decision-making required in AVs.</p><p>3. <strong>3D Vision</strong>: Technologies like LiDAR and 3D cameras are becoming more prevalent, providing depth information that 2D images cannot offer, thus improving the accuracy of object detection systems.</p><p>## Preparing for the Future: Skills, Technologies, and Knowledge</p><p>As computer vision technology advances, professionals in the field must continuously update their skills and knowledge:</p><p>1. <strong>Advanced Machine Learning</strong>: Deep learning will remain central to advancements in computer vision. Understanding complex neural networks and staying updated with the latest research is essential.</p><p>2. <strong>Software Proficiency</strong>: Skills in software and platforms such as TensorFlow, PyTorch, OpenCV, and CUDA are vital for implementing sophisticated vision algorithms effectively.</p><p>3. <strong>Interdisciplinary Knowledge</strong>: Knowledge in areas such as robotics, sensor fusion, and ethics will become increasingly important as these fields converge with computer vision.</p><p>4. <strong>Practical Experience</strong>: Hands-on experience through projects or collaboration with academia and industry can provide practical insights that are crucial for solving real-world problems.</p><p>5. <strong>Continued Learning</strong>: Engaging with the community through workshops, conferences, and courses is vital for keeping up with rapid technological changes.</p><p>In conclusion, as computer vision continues to evolve within the autonomous vehicle industry, adhering to best practices for safety and reliability, considering ethical implications, staying abreast of emerging trends, and continuously developing relevant skills are all essential for professionals in this exciting field.</p>
                      
                      <h3 id="best-practices-ethics-and-future-trends-adhering-to-best-practices-in-computer-vision-for-safety-and-reliability">Adhering to best practices in computer vision for safety and reliability</h3><h3 id="best-practices-ethics-and-future-trends-ethical-considerations-in-the-deployment-of-computer-vision-systems">Ethical considerations in the deployment of computer vision systems</h3><h3 id="best-practices-ethics-and-future-trends-emerging-trends-and-technologies-in-computer-vision-for-autonomous-vehicles">Emerging trends and technologies in computer vision for autonomous vehicles</h3><h3 id="best-practices-ethics-and-future-trends-preparing-for-the-future-skills-technologies-and-knowledge">Preparing for the future: skills, technologies, and knowledge</h3>
                  </section>
                  
                  

                
                <section id="conclusion">
                    <h2>Conclusion</h2>
                    <p>### Conclusion: Embracing the Future of Computer Vision in Autonomous Vehicles</p><p>In this tutorial, we have embarked on an enlightening journey through the realm of computer vision, exploring its pivotal role in the development and operation of autonomous vehicles. Starting with an <strong>introduction to computer vision</strong>, we have uncovered how this technology acts as the eyes of autonomous systems, enabling them to perceive and interact with their surroundings.</p><p><strong>Fundamentals of image processing</strong> laid the groundwork, helping us understand how machines interpret visual data. We progressed to <strong>object detection techniques</strong>, which are crucial for recognizing pedestrians, vehicles, and other critical elements in real-time. <strong>Semantic segmentation and depth estimation</strong> further refined our comprehension, illustrating how advanced algorithms provide a deeper understanding of scene context, which is vital for safe navigation.</p><p>Real-time processing capabilities are essential, as discussed in <strong>Real-Time Processing for Autonomous Driving</strong>. This ability ensures that autonomous vehicles can make immediate, informed decisions in dynamic environments. Moreover, we addressed <strong>best practices, ethics, and future trends</strong>, acknowledging the broader implications and responsibilities we bear in advancing these technologies.</p><p>As we conclude, remember that the journey does not end here. The field of computer vision in autonomous vehicles is rapidly evolving, with new challenges and breakthroughs emerging regularly. To continue advancing your knowledge, consider diving deeper into specialized areas such as neural networks or reinforcement learning. Online platforms like Coursera, edX, and others offer courses that can expand your expertise.</p><p>Lastly, I encourage you to apply the concepts learned here by participating in simulation projects or contributing to open-source initiatives. Practical application will not only reinforce your understanding but also help you make a tangible impact in this thrilling field. Let’s drive forward into a future where technology and innovation create safer, more efficient roads for everyone.</p>
                </section>
                

                
                <section id="code-examples">
                    <h2>Code Examples</h2>
                    
                    <div class="code-example">
                        <h3 id="code-example-1">Code Example</h3>
                        <p>This example demonstrates how to load an image, convert it to grayscale, and apply a Gaussian blur using OpenCV, which is a fundamental step in preprocessing images for computer vision tasks in autonomous vehicles.</p>
                        <pre><code class="language-python"># Importing the OpenCV library
import cv2

# Load an image from file
image = cv2.imread(&#39;path_to_image.jpg&#39;)

# Convert the image to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Apply Gaussian Blur to the image
gaussian_blur = cv2.GaussianBlur(gray_image, (5, 5), 0)

# Display the processed image
cv2.imshow(&#39;Grayscale with Gaussian Blur&#39;, gaussian_blur)
cv2.waitKey(0)
cv2.destroyAllWindows()</code></pre>
                        <p class="explanation">Run this script with a local image file path. It will display the image after converting it to grayscale and applying Gaussian blur. This preprocessing is common in vision tasks to reduce noise and improve feature extraction.</p>
                    </div>
                    
                    <div class="code-example">
                        <h3 id="code-example-2">Code Example</h3>
                        <p>This example demonstrates how to use the YOLO (You Only Look Once) model for real-time object detection, which is critical for identifying obstacles and other vehicles in autonomous driving.</p>
                        <pre><code class="language-python"># Load required libraries
import cv2
import numpy as np

# Load the YOLO network
net = cv2.dnn.readNet(&#39;yolov3.weights&#39;, &#39;yolov3.cfg&#39;)

# Load the COCO class labels our YOLO model was trained on
classes = []
with open(&#39;coco.names&#39;, &#39;r&#39;) as f:
    classes = [line.strip() for line in f.readlines()]

# Load image
frame = cv2.imread(&#39;path_to_image.jpg&#39;)
height, width, _ = frame.shape

# Prepare input for model
blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
net.setInput(blob)
outs = net.forward(net.getUnconnectedOutLayersNames())

# Display detection results
colors = np.random.uniform(0, 255, size=(len(classes), 3))
for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence &gt; 0.5:
            center_x = int(detection[0] * width)
            center_y = int(detection[1] * height)
            w = int(detection[2] * width)
            h = int(detection[3] * height)
            x = int(center_x - w / 2)
            y = int(center_y - h / 2)
            cv2.rectangle(frame, (x, y), (x + w, y + h), colors[class_id], 2)
            label = &#39;%s: %.2f&#39; % (classes[class_id], confidence)
            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[class_id], 2)
cv2.imshow(&#39;Image&#39;, frame)
cv2.waitKey(0)
cv2.destroyAllWindows()</code></pre>
                        <p class="explanation">Replace 'path_to_image.jpg', 'yolov3.weights', 'yolov3.cfg', and 'coco.names' with appropriate paths and files. The script will load an image, perform object detection with YOLO, and display the image with bounding boxes and labels around detected objects.</p>
                    </div>
                    
                    <div class="code-example">
                        <h3 id="code-example-3">Code Example</h3>
                        <p>This code example illustrates how to perform semantic segmentation using a pre-trained DeepLabV3 model, which is essential for understanding scene context in autonomous vehicles.</p>
                        <pre><code class="language-python"># Import necessary libraries
import torch
import torchvision.transforms as T
from PIL import Image
import matplotlib.pyplot as plt
from torchvision.models.segmentation import deeplabv3_resnet101

# Load a pre-trained DeepLabV3 model
model = deeplabv3_resnet101(pretrained=True)
model.eval()

# Load and preprocess an image
input_image = Image.open(&#39;path_to_image.jpg&#39;)
preprocess = T.Compose([
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0)

# Perform semantic segmentation
with torch.no_grad():
    output = model(input_batch)[&#39;out&#39;][0]
predictions = output.argmax(0)

# Display the segmentation results
plt.imshow(predictions.numpy())
plt.show()</code></pre>
                        <p class="explanation">Ensure you have PyTorch and torchvision installed. Replace 'path_to_image.jpg' with the path to a local image file. The script uses a pre-trained DeepLabV3 model to perform semantic segmentation and displays the segment map using matplotlib.</p>
                    </div>
                    
                </section>
                

                <div class="tutorial-rating">
                    <h3>Was this tutorial helpful?</h3>
                    <div class="stars">★ ★ ★ ★ ★</div>
                </div>

                <div class="related-tutorials">
                    <h3>Related Tutorials</h3>
                    <div class="tutorial-grid">
                        <a href="../tutorials/building-custom-gpt-applications.html" class="tutorial-card">
                            <img src="https://images.unsplash.com/photo-1555952494-efd681c7e3f9?ixlib=rb-4.0.3&auto=format&fit=crop&w=300&q=80" alt="Building Custom GPT Applications">
                            <h4>Building Custom GPT Applications</h4>
                        </a>
                        <a href="../tutorials/neural-networks-explained.html" class="tutorial-card">
                            <img src="https://images.unsplash.com/photo-1542281286-9e0a16bb7366?ixlib=rb-4.0.3&auto=format&fit=crop&w=300&q=80" alt="Neural Networks Explained">
                            <h4>Neural Networks Explained</h4>
                        </a>
                    </div>
                </div>
            </section>
        </article>

        <aside class="sidebar">
            <div class="newsletter-signup">
                <h3>Subscribe for More AI & ML Tutorials</h3>
                <p>Get weekly tutorials, guides, and AI resources delivered directly to your inbox.</p>
                <form>
                    <input type="email" placeholder="Enter your email">
                    <button type="submit">Subscribe</button>
                </form>
            </div>

            <div class="advertisement sidebar-ad">
                <h4>Your Ad Could Be Here</h4>
                <p>This is where a real sidebar advertisement would appear.</p>
                <a href="#" class="ad-link">Learn More</a>
            </div>

            <div class="category-info">
                <h3>Category</h3>
                <a href="../categories/computer-vision.html">Computer-vision</a>
            </div>

            <div class="popular-tutorials">
                <h3>Popular Tutorials</h3>
                <ul>
                    <li><a href="../tutorials/machine-learning-beginners-guide.html">Machine Learning: A Complete Beginner's Guide</a> 45K views</li>
                    <li><a href="../tutorials/pytorch-vs-tensorflow.html">PyTorch vs TensorFlow: Which One Should You Learn?</a> 32K views</li>
                    <li><a href="../tutorials/deep-learning-image-classification.html">Image Classification with Deep Learning</a> 28K views</li>
                    <li><a href="../tutorials/natural-language-processing-basics.html">NLP Basics: Understanding Language with AI</a> 24K views</li>
                </ul>
            </div>

            <div class="topics">
                <h3>Explore Topics</h3>
                <ul>
                    <li><a href="../categories/deep-learning.html">Deep Learning <span>42</span></a></li>
                    <li><a href="../categories/computer-vision.html">Computer Vision <span>38</span></a></li>
                    <li><a href="../categories/nlp.html">Natural Language Processing <span>35</span></a></li>
                    <li><a href="../categories/reinforcement-learning.html">Reinforcement Learning <span>24</span></a></li>
                    <li><a href="../categories/data-science.html">Data Science <span>56</span></a></li>
                    <li><a href="../categories/ai-ethics.html">AI Ethics <span>18</span></a></li>
                </ul>
            </div>

            <div class="advertisement sidebar-ad">
                <h4>Your Ad Could Be Here</h4>
                <p>This is where a real sidebar advertisement would appear.</p>
                <a href="#" class="ad-link">Learn More</a>
            </div>

            <div class="social-share">
                <h3>Share</h3>
                <div class="share-buttons">
                    <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fsolveforai.com%2Ftutorials%2Fcomputer-vision-for-autonomous-vehicles-an-introduction&text=Computer%20Vision%20for%20Autonomous%20Vehicles%3A%20An%20Introduction%20%7C%20Solve%20for%20AI" title="Share on Twitter">🐦</a>
                    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fsolveforai.com%2Ftutorials%2Fcomputer-vision-for-autonomous-vehicles-an-introduction" title="Share on Facebook">📘</a>
                    <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fsolveforai.com%2Ftutorials%2Fcomputer-vision-for-autonomous-vehicles-an-introduction&title=Computer%20Vision%20for%20Autonomous%20Vehicles%3A%20An%20Introduction%20%7C%20Solve%20for%20AI" title="Share on LinkedIn">💼</a>
                    <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fsolveforai.com%2Ftutorials%2Fcomputer-vision-for-autonomous-vehicles-an-introduction&title=Computer%20Vision%20for%20Autonomous%20Vehicles%3A%20An%20Introduction%20%7C%20Solve%20for%20AI" title="Share on Reddit">🔴</a>
                    <a href="mailto:?subject=Computer%20Vision%20for%20Autonomous%20Vehicles%3A%20An%20Introduction%20%7C%20Solve%20for%20AI&body=Check out this article: https%3A%2F%2Fsolveforai.com%2Ftutorials%2Fcomputer-vision-for-autonomous-vehicles-an-introduction" title="Share via Email">📧</a>
                </div>
            </div>
        </aside>
    </div>

    <footer>
        <div class="container">
            <p>&copy; 2025 Solve for AI. All rights reserved.</p>
        </div>
    </footer>

    <script src="../assets/js/main.js"></script>
</body>
</html>